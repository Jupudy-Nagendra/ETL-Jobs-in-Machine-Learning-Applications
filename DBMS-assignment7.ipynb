{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases and Data Warehouses Assignment - 7\n",
    "\n",
    "**Overview** :\n",
    "This assignment focuses on performing an end-to-end ETL (Extract, Transform, Load) process using the Titanic dataset from Kaggle. The ETL process is integrated with a simple machine-learning pipeline to demonstrate practical applications in a data science context.\n",
    "\n",
    "### Team Members (Group â€“ 08)\n",
    "\n",
    "##### 1.Rutika Rajesh Bankar - 25PGAI0103\n",
    "\n",
    "##### 2.Rishabh Gaur - 25PGAI0023\n",
    "\n",
    "##### 3.Mukesh Kumar Khemani - 25PGAI0115\n",
    "\n",
    "##### 4.Guna Shekhar Dasyam - 25PGAI0063\n",
    "\n",
    "##### 5.Nagendra Jupudy - 25PGAI0146\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Data Extraction\n",
    "**Objective:** \n",
    "#### Extract the Titanic dataset from Kaggle and load it into a pandas DataFrame.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "##### - Utilized the Kaggle API to download the dataset.\n",
    "##### - Loaded the dataset using pandas' read_csv function.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\\\Users\\\\india\\\\Desktop\\\\Programming_with_python\\\\Assignment_7_Database\\\\titanic\\\\train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].fillna(data['Age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Cabin'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Data Transformation\n",
    "**Objective** :\n",
    "Handle missing values, perform feature engineering, and prepare data by scaling numerical features and encoding categorical features for machine learning.\n",
    "\n",
    "**Steps and Tasks:**\n",
    "\n",
    "**Handling Missing Values:**\n",
    "\n",
    "##### - Filled missing values in 'Age' with the median.\n",
    "##### - Replaced missing values in 'Embarked' with the most common embarkation point.\n",
    "##### - Dropped the 'Cabin' column due to high missing values.\n",
    "\n",
    "**Feature Engineering**:\n",
    "\n",
    "##### - Extracted titles from passenger names.\n",
    "##### - Created a new feature 'FamilySize' based on the number of siblings/spouses and parents/children aboard.\n",
    "\n",
    "**Data Scaling and Encoding**:\n",
    "\n",
    "##### - Scaled numerical features (Age, Fare, FamilySize) using StandardScaler.\n",
    "##### - Encoded categorical features (Sex, Embarked, Title) using OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title'] = data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FamilySize'] = data['SibSp'] + data['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['Age', 'Fare', 'FamilySize']\n",
    "categorical_cols = ['Sex', 'Embarked', 'Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "('num', numerical_transformer, numerical_cols),\n",
    "('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed = preprocessor.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col_names = numerical_cols\n",
    "categorical_col_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
    "transformed_columns = numerical_col_names + categorical_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data_transformed:\", data_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed columns:\", transformed_columns)\n",
    "print(\"Length of transformed_columns:\", len(transformed_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_transformed.shape[1] != len(transformed_columns):\n",
    "    raise ValueError(\"Mismatch between number of columns in data_transformed and number of feature names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed_df = pd.DataFrame(data = data_transformed.todense(), columns=transformed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Data Loading\n",
    "**Objective**:\n",
    "Store the cleaned and transformed data in an SQLite database and demonstrate data retrieval.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "Utilized SQLAlchemy to create an SQLite database and stored the transformed data.\n",
    "Implemented a function to retrieve data from the database to verify successful storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///titanic.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed_df.to_sql('titanic_transformed', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from the database\n",
    "def load_data_from_db(engine):\n",
    "    query = \"SELECT * FROM titanic_transformed\"\n",
    "    data_from_db = pd.read_sql(query, engine)\n",
    "    return data_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_loaded = load_data_from_db(engine)\n",
    "print(data_loaded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Integration with ML Pipeline\n",
    "**Objective**:\n",
    "Build and evaluate a logistic regression model using the transformed and loaded data.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "Split data into training and testing sets.\n",
    "Trained a logistic regression model and evaluated its accuracy on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variable\n",
    "X = data_loaded\n",
    "y = data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation and Code Quality**\n",
    "##### - Detailed comments and clear code structure are maintained throughout the scripts to ensure readability and maintainability.\n",
    "##### - Included a README.md file with environment setup and script execution instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "#### This assignment effectively demonstrates the capability to perform an ETL job integrated with a machine learning pipeline, highlighting practical data science skills from data extraction to model evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
